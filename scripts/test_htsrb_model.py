#!/usr/bin/env python3
"""
æµ‹è¯• HTSRB (Hong Kong Traffic Sign Recognition Benchmark) æ¨¡å‹

æ¨¡å‹ä¿¡æ¯ï¼š
- æ¶æ„: ViT-B/16 (Vision Transformer Base, patch size 16)
- ç±»åˆ«æ•°: 66 (0-64: 65ç§äº¤é€šæ ‡å¿—, 65: other)
- æµ‹è¯•å‡†ç¡®ç‡: 92.8%
- è¾“å…¥å°ºå¯¸: 224x224
"""

import json
import torch
import torch.nn as nn
from torchvision import transforms
from torchvision.models import vit_b_16, ViT_B_16_Weights
from PIL import Image
from pathlib import Path
import sys

# è·¯å¾„é…ç½®
BASE_DIR = Path(__file__).parent.parent
MODEL_PATH = BASE_DIR / "htsrb_repo" / "vit_b_16_best_layer8.pth"
LABEL_MAPPING_PATH = BASE_DIR / "htsrb_repo" / "label_mapping.json"


def load_label_mapping():
    """åŠ è½½æ ‡ç­¾æ˜ å°„"""
    with open(LABEL_MAPPING_PATH, "r", encoding="utf-8") as f:
        mapping = json.load(f)
    # è½¬æ¢ä¸º listï¼ŒæŒ‰ index æ’åº
    idx_to_label = mapping["idx_to_label"]
    num_classes = mapping["num_classes"]
    labels = [idx_to_label[str(i)] for i in range(num_classes)]
    return labels


# åŠ è½½ç±»åˆ«æ ‡ç­¾
CLASS_LABELS = load_label_mapping()


def load_model(model_path: Path, num_classes: int = 66):
    """åŠ è½½ ViT-B/16 æ¨¡å‹"""
    # åˆ›å»º ViT-B/16 æ¨¡å‹
    model = vit_b_16(weights=None)

    # æ›¿æ¢åˆ†ç±»å¤´ä»¥åŒ¹é… 66 ç±»
    model.heads.head = nn.Linear(768, num_classes)

    # åŠ è½½æƒé‡
    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])

    model.eval()
    print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    print(f"  Epoch: {checkpoint['epoch']}")
    print(f"  Test Acc: {checkpoint['test_acc']:.2f}%")

    return model


def get_transform():
    """è·å–å›¾åƒé¢„å¤„ç† transform"""
    return transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])


def predict_image(model, image_path: Path, transform, top_k: int = 5):
    """é¢„æµ‹å•å¼ å›¾ç‰‡"""
    # åŠ è½½å›¾ç‰‡
    image = Image.open(image_path).convert('RGB')

    # é¢„å¤„ç†
    input_tensor = transform(image).unsqueeze(0)  # [1, 3, 224, 224]

    # æ¨ç†
    with torch.no_grad():
        outputs = model(input_tensor)
        probabilities = torch.softmax(outputs, dim=1)[0]

    # è·å– top-k é¢„æµ‹
    top_probs, top_indices = torch.topk(probabilities, top_k)

    results = []
    for prob, idx in zip(top_probs, top_indices):
        results.append({
            'class_id': idx.item(),
            'class_name': CLASS_LABELS[idx.item()],
            'probability': prob.item()
        })

    return results


def main():
    import argparse
    parser = argparse.ArgumentParser(description="æµ‹è¯• HTSRB æ¨¡å‹")
    parser.add_argument("images", nargs="+", help="è¦æµ‹è¯•çš„å›¾ç‰‡è·¯å¾„")
    parser.add_argument("--top-k", type=int, default=5, help="æ˜¾ç¤ºå‰ k ä¸ªé¢„æµ‹ç»“æœ")
    args = parser.parse_args()

    # åŠ è½½æ¨¡å‹
    print("=" * 50)
    print("åŠ è½½ HTSRB æ¨¡å‹...")
    model = load_model(MODEL_PATH)
    transform = get_transform()
    print("=" * 50)

    # é¢„æµ‹æ¯å¼ å›¾ç‰‡
    for image_path in args.images:
        path = Path(image_path)
        if not path.exists():
            print(f"\nâŒ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            continue

        print(f"\nğŸ“· {path.name}")
        print("-" * 40)

        results = predict_image(model, path, transform, args.top_k)

        for i, r in enumerate(results):
            prob_bar = "â–ˆ" * int(r['probability'] * 20)
            print(f"  {i+1}. [{r['class_id']:2d}] {r['class_name']:<30} {r['probability']*100:5.1f}% {prob_bar}")


if __name__ == "__main__":
    main()
