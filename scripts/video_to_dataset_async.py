#!/usr/bin/env python3
"""
ğŸš€ å¼‚æ­¥ç‰ˆæœ¬ - è§†é¢‘åˆ°æ•°æ®é›†æµæ°´çº¿

ä½¿ç”¨ asyncio + httpx å®ç°çœŸæ­£çš„å¹¶å‘ API è¯·æ±‚ï¼Œé€Ÿåº¦æ›´å¿«ã€‚

ç”¨æ³•:
    # åŸºæœ¬ç”¨æ³• (é»˜è®¤ PNG æ— æŸæŠ½å¸§)
    python3 scripts/video_to_dataset_async.py --video D4.1 --workers 15

    # ä½¿ç”¨ JPEG æŠ½å¸§ï¼ˆçœç©ºé—´ï¼‰
    python3 scripts/video_to_dataset_async.py --video D4.1 --jpeg-frames

    # ä½¿ç”¨ Gemini æ¨¡å‹
    python3 scripts/video_to_dataset_async.py --video D4.1 --model gemini-2.5-flash
"""

import os
import sys
import json
import argparse
import subprocess
import time
import asyncio
import base64
from pathlib import Path
from typing import List, Dict, Any, Optional

import httpx
from PIL import Image

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from glm_labeling.utils.labels import get_category, normalize_vehicle_label
from glm_labeling.utils.json_utils import parse_llm_json
from glm_labeling.core.sign_classifier_v2 import SignClassifierV2

# DINOv2 åˆ†ç±»å™¨ï¼ˆæ¨èï¼Œæœ€å¼ºç‰¹å¾æå–ï¼‰
try:
    from scripts.dinov2_classifier import DINOv2SignClassifier
    DINOV2_AVAILABLE = True
except ImportError:
    DINOV2_AVAILABLE = False

# CLIP åˆ†ç±»å™¨ï¼ˆå¯é€‰ï¼‰
try:
    from scripts.clip_rag_classifier import CLIPSignClassifier
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False


# ============================================================================
# é…ç½®
# ============================================================================

# å¤šæ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    # GLM ç³»åˆ—
    "glm": {
        "name": "glm-4.6v",
        "api_base": "https://api.z.ai/api/paas/v4",
        "api_key_env": "ZAI_API_KEY",
        "coord_base": 1000,
        "type": "glm",
    },
    # Gemini ç³»åˆ— - æ¨è
    "gemini-2.5-flash": {
        "name": "gemini-2.5-flash",
        "api_base": None,
        "api_key_env": "GOOGLE_API_KEY",
        "coord_base": 1000,
        "type": "gemini",
    },
    "gemini-2.0-flash": {
        "name": "gemini-2.0-flash",
        "api_base": None,
        "api_key_env": "GOOGLE_API_KEY",
        "coord_base": 1000,
        "type": "gemini",
    },
    "gemini-2.5-pro": {
        "name": "gemini-2.5-pro",
        "api_base": None,
        "api_key_env": "GOOGLE_API_KEY",
        "coord_base": 1000,
        "type": "gemini",
    },
    "gemini-3-pro": {
        "name": "gemini-3-pro-preview",
        "api_base": None,
        "api_key_env": "GOOGLE_API_KEY",
        "coord_base": 1000,
        "type": "gemini",
    },
    # å…¼å®¹æ—§å‚æ•°
    "gemini": {
        "name": "gemini-2.5-flash",  # é»˜è®¤ä½¿ç”¨ 2.5-flash
        "api_base": None,
        "api_key_env": "GOOGLE_API_KEY",
        "coord_base": 1000,
        "type": "gemini",
    },
}

# é»˜è®¤æ¨¡å‹
DEFAULT_MODEL = "glm"
COORD_BASE = 1000

VIDEO_DIR = Path("raw_data/videos/clips")  # é»˜è®¤æŸ¥æ‰¾åˆ‡åˆ†åçš„ç‰‡æ®µ
DATASET_OUTPUT = Path("dataset_output")  # ç›´æ¥è¾“å‡ºåˆ°æœ€ç»ˆç›®å½•

# 188 ç§äº¤é€šæ ‡å¿—å€™é€‰åº“
SIGNS_DIR = Path("raw_data/signs")

def load_sign_candidates():
    """ä»æ ‡å¿—å›¾ç‰‡ç›®å½•åŠ¨æ€åŠ è½½æ‰€æœ‰æ ‡å¿—åç§°ï¼ˆ188ç§ï¼‰"""
    if not SIGNS_DIR.exists():
        print(f"âš ï¸ æ‰¾ä¸åˆ°æ ‡å¿—ç›®å½•: {SIGNS_DIR}")
        return []
    return [f.stem for f in sorted(SIGNS_DIR.glob("*.png"))]

ALL_SIGN_CANDIDATES = load_sign_candidates()

COLORS = {
    'traffic_sign': (0, 100, 255),    # ID 0: è“è‰²
    'pedestrian': (255, 0, 0),        # ID 1: çº¢è‰²
    'vehicle': (0, 255, 0),           # ID 2: ç»¿è‰²
    'small_obstacle': (255, 165, 0),  # ID 3: æ©™è‰²
}

DETECTION_PROMPT = """è¯·æ£€æµ‹å›¾ç‰‡ä¸­çš„ä»¥ä¸‹4ç±»ç‰©ä½“ï¼Œè¿”å›JSONæ ¼å¼ã€‚

## é‡è¦æ’é™¤è§„åˆ™ï¼š
â›” ä¸è¦æ ‡æ³¨ç¬¬ä¸€äººç§°è§†è§’ä¸‹è‡ªå·±éª‘çš„è½¦ï¼ˆæ‘©æ‰˜è½¦/ç”µåŠ¨è½¦/è‡ªè¡Œè½¦çš„è½¦æŠŠã€ä»ªè¡¨ç›˜ã€æ‰‹è‡‚ç­‰ï¼‰ï¼

## æ£€æµ‹ç±»åˆ«ä¸ç»†ç²’åº¦è¦æ±‚ï¼š

### 1. è¡Œäººç±» (pedestrian) - 2ç§æ ‡ç­¾
- pedestrian: å•ä¸ªæˆ–å°‘é‡è¡Œäºº
- crowd: äººç¾¤ï¼ˆå¤šäººèšé›†ï¼‰

### 2. è½¦è¾†ç±» (vehicle) - 5ç§æ ‡ç­¾
ç»Ÿä¸€ä½¿ç”¨ vehicleï¼ŒåªåŒºåˆ†è¡Œé©¶çŠ¶æ€ï¼š

**ğŸš¨ çŠ¶æ€åˆ¤æ–­è§„åˆ™ï¼ˆæ ¸å¿ƒï¼šå…³æ³¨å°¾ç¯ï¼æŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š**
1. **åˆ¹è½¦çŠ¶æ€**: å°¾ç¯æ˜æ˜¾å˜äº®ã€çº¢è‰²åˆ¹è½¦ç¯äº®èµ· â†’ `vehicle_braking`
2. **åŒé—ªçŠ¶æ€**: å·¦å³ä¸¤ä¾§è½¬å‘ç¯åŒæ—¶äº®èµ·/é—ªçƒ â†’ `vehicle_double_flash`
3. **å³è½¬çŠ¶æ€**: å³ä¾§è½¬å‘ç¯äº®ï¼ˆé»„è‰²/ç¥ç€è‰²ï¼‰æˆ–æ˜æ˜¾å³è½¬å¼¯ â†’ `vehicle_turning_right`
4. **å·¦è½¬çŠ¶æ€**: å·¦ä¾§è½¬å‘ç¯äº®ï¼ˆé»„è‰²/ç¥ç€è‰²ï¼‰æˆ–æ˜æ˜¾å·¦è½¬å¼¯ â†’ `vehicle_turning_left`
5. **æ­£å¸¸çŠ¶æ€**: ç›´è¡Œæˆ–æ— ç¯å…‰ä¿¡å· â†’ `vehicle`

âš ï¸ æ³¨æ„ï¼šä»…é“è·¯å¼¯æ›²ä½†è½¦è¾†æ­£å¸¸è¡Œé©¶ã€æ²¡æœ‰æ‰“ç¯ â†’ æ ‡ä¸º `vehicle`ï¼ˆç›´è¡Œï¼‰

### 3. äº¤é€šæ ‡å¿—ç±» (traffic_sign)
traffic_sign

### 4. å°å‹éšœç¢ç‰©ç±» (small_obstacle)
traffic_cone, construction_barrier

## è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
[
  {"label": "vehicle_braking", "bbox_2d": [100, 200, 300, 400]},
  {"label": "vehicle_double_flash", "bbox_2d": [400, 300, 600, 500]},
  {"label": "traffic_sign", "bbox_2d": [50, 50, 80, 80]}
]

å¦‚æœæ²¡æœ‰ç›®æ ‡ï¼Œè¿”å› []
åªè¿”å›JSONæ•°ç»„ï¼"""


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

import re
import uuid


def image_to_base64_url(image_path: str) -> str:
    """å°†å›¾ç‰‡è½¬ä¸º base64 data URL"""
    with open(image_path, "rb") as f:
        data = base64.b64encode(f.read()).decode("utf-8")
    
    ext = Path(image_path).suffix.lower()
    mime = {"jpg": "jpeg", "jpeg": "jpeg", "png": "png", "gif": "gif", "webp": "webp"}
    mime_type = mime.get(ext.lstrip("."), "jpeg")
    
    return f"data:image/{mime_type};base64,{data}"


def get_image_size(image_path: str) -> tuple:
    """è·å–å›¾ç‰‡å°ºå¯¸"""
    with Image.open(image_path) as img:
        return img.size  # (width, height)


def convert_coords(bbox: List[int], width: int, height: int) -> List[int]:
    """å°† GLM å½’ä¸€åŒ–åæ ‡ (0-1000) è½¬ä¸ºåƒç´ åæ ‡"""
    x1, y1, x2, y2 = bbox
    return [
        int(x1 * width / COORD_BASE),
        int(y1 * height / COORD_BASE),
        int(x2 * width / COORD_BASE),
        int(y2 * height / COORD_BASE)
    ]


def to_xanylabeling_format(detections: List[Dict], image_path: str) -> Dict:
    """è½¬æ¢ä¸º X-AnyLabeling æ ¼å¼"""
    width, height = get_image_size(image_path)
    
    shapes = []
    for det in detections:
        bbox = det["bbox"]
        shapes.append({
            "label": det["label"],
            "points": [[bbox[0], bbox[1]], [bbox[2], bbox[3]]],
            "shape_type": "rectangle",
            "flags": {"category": det["category"]}
        })
    
    return {
        "version": "0.4.0",
        "flags": {},
        "shapes": shapes,
        "imagePath": Path(image_path).name,
        "imageHeight": height,
        "imageWidth": width
    }


# ============================================================================
# å¼‚æ­¥ API è°ƒç”¨
# ============================================================================

class AsyncDetector:
    """å¼‚æ­¥ç›®æ ‡æ£€æµ‹å™¨ - æ”¯æŒå¤šæ¨¡å‹ (GLM / Gemini) å’Œå¤šç§ RAG åˆ†ç±»å™¨"""
    
    def __init__(self, api_key: str, max_concurrent: int = 12, timeout: float = 45.0, model_type: str = "glm", 
                 use_clip_rag: bool = False, sign_classifier_type: str = None):
        """
        Args:
            api_key: API å¯†é’¥
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            timeout: è¶…æ—¶æ—¶é—´
            model_type: VLM æ£€æµ‹æ¨¡å‹ç±»å‹
            use_clip_rag: (æ—§å‚æ•°ï¼Œå…¼å®¹) æ˜¯å¦ä½¿ç”¨ CLIP åˆ†ç±»
            sign_classifier_type: äº¤é€šæ ‡å¿—åˆ†ç±»å™¨ç±»å‹ ("dinov2", "clip", "vlm", None)
                - None/vlm: ä½¿ç”¨ VLM æ–‡å­—é€‰é¡¹ï¼ˆé»˜è®¤ï¼‰
                - dinov2: ä½¿ç”¨ DINOv2 å‘é‡æ£€ç´¢ï¼ˆæ¨èï¼Œæœ€å¼ºï¼‰
                - clip: ä½¿ç”¨ CLIP å‘é‡æ£€ç´¢
        """
        self.api_key = api_key
        self.timeout = timeout
        self.model_type = model_type
        self.model_config = MODEL_CONFIGS.get(model_type, MODEL_CONFIGS["glm"])
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.client: Optional[httpx.AsyncClient] = None
        self.gemini_client = None
        
        # ç¡®å®šåˆ†ç±»å™¨ç±»å‹ï¼ˆå…¼å®¹æ—§å‚æ•°ï¼‰
        if sign_classifier_type:
            self.sign_classifier_type = sign_classifier_type
        elif use_clip_rag:
            self.sign_classifier_type = "clip"
        else:
            self.sign_classifier_type = "vlm"
        
        # äº¤é€šæ ‡å¿—åˆ†ç±»å™¨åˆå§‹åŒ–
        self.vector_classifier = None
        self.sign_classifier = None
        
        if self.sign_classifier_type == "dinov2":
            if DINOV2_AVAILABLE:
                print("   ğŸ¦– ä½¿ç”¨ DINOv2 å‘é‡æ£€ç´¢è¿›è¡Œäº¤é€šæ ‡å¿—åˆ†ç±»ï¼ˆæ¨èï¼‰")
                self.vector_classifier = DINOv2SignClassifier(use_69_signs=False)
            else:
                print("   âš ï¸ DINOv2 ä¸å¯ç”¨ï¼Œå›é€€åˆ° VLM æ–¹å¼")
                self.sign_classifier_type = "vlm"
        elif self.sign_classifier_type == "clip":
            if CLIP_AVAILABLE:
                print("   ğŸ“ ä½¿ç”¨ CLIP å‘é‡æ£€ç´¢è¿›è¡Œäº¤é€šæ ‡å¿—åˆ†ç±»")
                self.vector_classifier = CLIPSignClassifier(use_69_signs=False)
            else:
                print("   âš ï¸ CLIP ä¸å¯ç”¨ï¼Œå›é€€åˆ° VLM æ–¹å¼")
                self.sign_classifier_type = "vlm"
        
        if self.sign_classifier_type == "vlm":
            # ä½¿ç”¨åŸç‰ˆ VLM åˆ†ç±»å™¨ï¼ˆ69ä¸ªæ ¸å¿ƒæ ‡å¿— + otherï¼‰
            self.sign_classifier = SignClassifierV2(
                api_key=api_key, 
                timeout=timeout,
                model_choice=model_type
            )
    
    async def __aenter__(self):
        model_type = self.model_config.get("type", self.model_type)
        if model_type == "gemini":
            # ä½¿ç”¨ google-genai SDK
            try:
                from google import genai
                self.gemini_client = genai.Client()
                print(f"   ğŸ”Œ Gemini å®¢æˆ·ç«¯å·²åˆå§‹åŒ– ({self.model_config['name']})")
            except ImportError:
                raise ImportError("è¯·å®‰è£… google-genai: pip install google-genai")
        else:
            # ä½¿ç”¨ httpx å®¢æˆ·ç«¯ (GLM)
            self.client = httpx.AsyncClient(
                base_url=self.model_config["api_base"],
                timeout=httpx.Timeout(self.timeout, connect=10.0),
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                limits=httpx.Limits(max_connections=30, max_keepalive_connections=15)
            )
        return self
    
    async def __aexit__(self, *args):
        if self.client:
            await self.client.aclose()
    
    async def detect(self, image_path: str, retry: int = 3) -> tuple:
        """
        å¼‚æ­¥æ£€æµ‹å•å¼ å›¾ç‰‡
        
        Returns:
            (detections, error)
        """
        async with self.semaphore:  # æ§åˆ¶å¹¶å‘
            return await self._detect_with_retry(image_path, retry)
    
    async def _detect_with_retry(self, image_path: str, max_retry: int) -> tuple:
        """å¸¦é‡è¯•çš„æ£€æµ‹ - æ”¯æŒå¤šæ¨¡å‹"""
        image_name = Path(image_path).name
        last_error = None
        width, height = get_image_size(image_path)
        
        for attempt in range(max_retry):
            try:
                model_type = self.model_config.get("type", self.model_type)
                if model_type == "gemini":
                    # ============ Gemini API ============
                    content = await self._detect_gemini(image_path)
                else:
                    # ============ GLM API ============
                    content = await self._detect_glm(image_path)
                
                # è§£æ JSON
                detections = parse_llm_json(content)
                
                if detections is None:
                    last_error = "JSON parse error"
                    await asyncio.sleep(1)
                    continue
                
                if not detections:
                    return [], None
                
                # åå¤„ç†
                processed = []
                for det in detections:
                    if "label" not in det or "bbox_2d" not in det:
                        continue
                    
                    bbox = convert_coords(det["bbox_2d"], width, height)
                    label = det["label"].lower().replace(" ", "_").replace("-", "_")
                    category = get_category(label)
                    
                    if category == "vehicle":
                        label = normalize_vehicle_label(label)
                    
                    processed.append({
                        "label": label,
                        "category": category,
                        "bbox": bbox
                    })
                
                return processed, None
                
            except Exception as e:
                last_error = str(e)[:50]
                if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                    await asyncio.sleep(3 * (attempt + 1))
                else:
                    await asyncio.sleep(2 * (attempt + 1))
        
        return [], last_error
    
    async def _detect_glm(self, image_path: str) -> str:
        """GLM API æ£€æµ‹"""
        base64_url = image_to_base64_url(image_path)
        
        payload = {
            "model": self.model_config["name"],
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": base64_url}},
                    {"type": "text", "text": DETECTION_PROMPT}
                ]
            }]
        }
        
        response = await self.client.post("/chat/completions", json=payload)
        
        if response.status_code == 429:
            raise Exception("429 Rate Limited")
        
        response.raise_for_status()
        data = response.json()
        return data["choices"][0]["message"]["content"]
    
    async def _detect_gemini(self, image_path: str) -> str:
        """Gemini API æ£€æµ‹"""
        from google.genai import types
        
        # è¯»å–å›¾ç‰‡
        with open(image_path, "rb") as f:
            image_data = f.read()
        
        ext = Path(image_path).suffix.lower()
        mime_types = {".jpg": "image/jpeg", ".jpeg": "image/jpeg", ".png": "image/png"}
        mime_type = mime_types.get(ext, "image/jpeg")
        
        # è°ƒç”¨ Gemini (åŒæ­¥è°ƒç”¨ï¼Œåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: self.gemini_client.models.generate_content(
                model=self.model_config["name"],
                contents=[
                    types.Part.from_bytes(data=image_data, mime_type=mime_type),
                    DETECTION_PROMPT
                ]
            )
        )
        
        return response.text
    
    async def classify_sign_rag(self, image_path: str, bbox: list) -> str:
        """
        RAG äº¤é€šæ ‡å¿—ç²¾æ’ï¼ˆå¼‚æ­¥ç‰ˆï¼‰- æ”¯æŒå¤šç§åˆ†ç±»å™¨
        
        DINOv2 æ¨¡å¼ï¼ˆæ¨èï¼‰ï¼š
        - ä½¿ç”¨ DINOv2 å¼ºå¤§çš„è§†è§‰ç‰¹å¾ + Chroma å‘é‡æ•°æ®åº“
        - ç»†ç²’åº¦ç‰¹å¾æå–ï¼Œå¯¹å¤æ‚åœºæ™¯æ›´é²æ£’
        
        CLIP æ¨¡å¼ï¼š
        - ä½¿ç”¨ CLIP å›¾åƒå‘é‡ + Chroma å‘é‡æ•°æ®åº“
        - é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢æœ€ç›¸ä¼¼çš„æ ‡å¿—
        
        VLM æ¨¡å¼ï¼ˆåŸç‰ˆï¼‰ï¼š
        - ä½¿ç”¨ VLM + æ–‡å­—é€‰é¡¹åˆ—è¡¨
        - ä¾èµ–æ¨¡å‹é¢„è®­ç»ƒçŸ¥è¯†
        
        Args:
            image_path: åŸå›¾è·¯å¾„
            bbox: äº¤é€šæ ‡å¿—çš„è¾¹ç•Œæ¡† [x1, y1, x2, y2]
        
        Returns:
            ç»†ç²’åº¦æ ‡ç­¾ï¼Œæˆ– "other"ï¼ˆå¯¼èˆª/æ–¹å‘ç‰Œç­‰ï¼‰
        """
        try:
            if self.vector_classifier:
                # å‘é‡æ£€ç´¢æ¨¡å¼ï¼ˆDINOv2 æˆ– CLIPï¼‰
                loop = asyncio.get_event_loop()
                label, score = await loop.run_in_executor(
                    None,
                    lambda: self.vector_classifier.classify(image_path, bbox)
                )
                return label
            else:
                # VLM æ–‡å­—é€‰é¡¹æ¨¡å¼ï¼ˆåŸç‰ˆï¼‰
                label, description, raw = await self.sign_classifier.classify(image_path, bbox)
                return label
        except Exception as e:
            return "traffic_sign"


# ============================================================================
# Step 1: æŠ½å¸§
# ============================================================================

def extract_frames(video_path: str, output_name: str, dataset_dir: Path, fps: int = 3, lossless: bool = False, keyframes_only: bool = True) -> tuple:
    """ä»è§†é¢‘æŠ½å¸§ï¼Œç›´æ¥è¾“å‡ºåˆ° dataset ç›®å½•

    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_name: è¾“å‡ºåç§°ï¼ˆç”¨äºå‘½åå¸§ï¼‰
        dataset_dir: ç›®æ ‡ dataset ç›®å½•
        fps: æŠ½å¸§ç‡
        lossless: æ˜¯å¦æ— æŸè¾“å‡º (PNG æ ¼å¼)
        keyframes_only: æ˜¯å¦åªæå– I-frame å…³é”®å¸§ï¼ˆé»˜è®¤ Trueï¼Œç”»è´¨æœ€é«˜ï¼‰
    """
    video_path = Path(video_path)

    if not video_path.exists():
        print(f"âŒ è§†é¢‘ä¸å­˜åœ¨: {video_path}")
        return None, 0

    frames_dir = dataset_dir / "frames"
    frames_dir.mkdir(parents=True, exist_ok=True)

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¸§ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼Œæ”¯æŒ jpg å’Œ pngï¼‰
    existing_frames = list(frames_dir.glob("*.jpg")) + list(frames_dir.glob("*.png"))
    if existing_frames:
        print(f"\nğŸ“¹ Step 1: æŠ½å¸§ (å·²å­˜åœ¨ {len(existing_frames)} å¸§ï¼Œè·³è¿‡)")
        return frames_dir, len(existing_frames)

    format_desc = "PNG æ— æŸ" if lossless else "JPEG q=2"
    keyframe_desc = "ä»…å…³é”®å¸§(I-frame)" if keyframes_only else "æ‰€æœ‰å¸§"
    print(f"\nğŸ“¹ Step 1: æŠ½å¸§ ({fps} FPS, {format_desc}, {keyframe_desc})")
    print(f"   è§†é¢‘: {video_path}")
    print(f"   ç›®æ ‡: {frames_dir}")

    # æ„å»ºè§†é¢‘æ»¤é•œ
    if keyframes_only:
        # åªæå– I-frame å…³é”®å¸§ï¼Œå†ç”¨ fps è¿‡æ»¤
        vf_filter = f"select='eq(pict_type,I)',fps={fps}"
    else:
        # æå–æ‰€æœ‰å¸§ï¼ŒæŒ‰ fps é‡‡æ ·
        vf_filter = f"fps={fps}"

    if lossless:
        # PNG æ— æŸè¾“å‡º
        output_pattern = str(frames_dir / f"{output_name}_%06d.png")
        cmd = [
            "ffmpeg", "-i", str(video_path),
            "-vf", vf_filter,
            "-vsync", "vfr",
            output_pattern,
            "-y"
        ]
    else:
        # JPEG é«˜è´¨é‡è¾“å‡º (q=2)
        output_pattern = str(frames_dir / f"{output_name}_%06d.jpg")
        cmd = [
            "ffmpeg", "-i", str(video_path),
            "-vf", vf_filter,
            "-vsync", "vfr",
            "-q:v", "2",
            output_pattern,
            "-y"
        ]

    try:
        subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        frame_count = len(list(frames_dir.glob("*.jpg")) + list(frames_dir.glob("*.png")))
        print(f"   âœ… æŠ½å– {frame_count} å¸§")
        return frames_dir, frame_count
    except Exception as e:
        print(f"   âŒ ffmpeg é”™è¯¯: {e}")
        return None, 0


# ============================================================================
# Step 2: å¼‚æ­¥æ ‡æ³¨
# ============================================================================

async def run_labeling_async(
    frames_dir: Path, 
    dataset_dir: Path,
    video_name: str, 
    workers: int,
    api_key: str,
    use_rag: bool = True,
    model_type: str = "glm",
    use_clip_rag: bool = False,
    sign_classifier_type: str = None
) -> Path:
    """å¼‚æ­¥è¿è¡Œæ ‡æ³¨ï¼Œç›´æ¥è¾“å‡ºåˆ° dataset ç›®å½• - æ”¯æŒå¤šæ¨¡å‹å’Œå¤šç§åˆ†ç±»å™¨"""
    # ç¡®å®šåˆ†ç±»å™¨æ˜¾ç¤ºåç§°
    if sign_classifier_type == "dinov2":
        rag_status = "ğŸ¦– DINOv2"
    elif sign_classifier_type == "clip" or use_clip_rag:
        rag_status = "ğŸ“ CLIP"
    elif use_rag:
        rag_status = "âœ… VLM"
    else:
        rag_status = "âŒ ç¦ç”¨"
    
    model_name = MODEL_CONFIGS.get(model_type, MODEL_CONFIGS["glm"])["name"]
    print(f"\nğŸ·ï¸ Step 2: å¼‚æ­¥æ ‡æ³¨")
    print(f"   å¹¶å‘æ•°: {workers} | æ¨¡å‹: {model_name} | RAG: {rag_status}")
    
    # æ”¯æŒ jpg å’Œ png ä¸¤ç§æ ¼å¼
    image_files = sorted(list(frames_dir.glob("*.jpg")) + list(frames_dir.glob("*.png")))
    if not image_files:
        print("   âŒ æ²¡æœ‰æ‰¾åˆ°å¸§")
        return None
    
    output_dir = dataset_dir / "annotations"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è¿‡æ»¤å·²å¤„ç†çš„ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    todo_files = []
    for img in image_files:
        json_path = output_dir / f"{img.stem}.json"
        if not json_path.exists():
            todo_files.append(img)
    
    skipped = len(image_files) - len(todo_files)
    if skipped > 0:
        print(f"   ğŸ“Œ æ–­ç‚¹ç»­ä¼ : è·³è¿‡ {skipped} å¼ å·²å¤„ç†")
    
    if not todo_files:
        print("   âœ… æ‰€æœ‰å¸§å·²å¤„ç†å®Œæˆ")
        return output_dir
    
    print(f"   ğŸ“ å¾…å¤„ç†: {len(todo_files)} å¸§")
    
    start_time = time.time()
    stats = {"traffic_sign": 0, "pedestrian": 0, "vehicle": 0, "small_obstacle": 0}
    success = 0
    errors = 0
    
    async with AsyncDetector(api_key, max_concurrent=workers, model_type=model_type, 
                             use_clip_rag=use_clip_rag, sign_classifier_type=sign_classifier_type) as detector:
        # åˆ›å»ºä»»åŠ¡å¹¶è®°å½•å¯¹åº”çš„æ–‡ä»¶
        tasks = {
            asyncio.create_task(
                detect_and_save(detector, str(img), output_dir, stats, use_rag=use_rag)
            ): img for img in todo_files
        }
        
        total = len(image_files)
        completed = 0
        
        # å®æ—¶è¾“å‡ºï¼šä»»åŠ¡å®Œæˆä¸€ä¸ªå°±è¾“å‡ºä¸€ä¸ª
        for coro in asyncio.as_completed(tasks.keys()):
            completed += 1
            idx = skipped + completed
            
            try:
                result = await coro
                
                if result[1]:  # error
                    print(f"  âš ï¸ [{idx}/{total}] {result[1]}", flush=True)
                    errors += 1
                else:
                    count = result[0]
                    emoji = "âœ…" if count > 0 else "âšª"
                    print(f"  {emoji} [{idx}/{total}] {count} objects", flush=True)
                    success += 1
                    
            except Exception as e:
                print(f"  âŒ [{idx}/{total}] {e}", flush=True)
                errors += 1
    
    elapsed = time.time() - start_time
    print(f"\n   ğŸ“Š ç»Ÿè®¡: {dict(stats)}")
    print(f"   â±ï¸ è€—æ—¶: {elapsed:.1f}s ({elapsed/len(todo_files):.2f}s/å¸§)")
    print(f"   âœ… æˆåŠŸ: {success} | âŒ é”™è¯¯: {errors}")
    
    return output_dir


async def detect_and_save(
    detector: AsyncDetector,
    image_path: str,
    output_dir: Path,
    stats: dict,
    use_rag: bool = True
) -> tuple:
    """æ£€æµ‹å¹¶ä¿å­˜ç»“æœ"""
    detections, error = await detector.detect(image_path)
    
    if error:
        return (0, error)
    
    # RAG ç»†ç²’åº¦åˆ†ç±»ï¼ˆäº¤é€šæ ‡å¿—ï¼‰
    if use_rag:
        for det in detections:
            if det.get("category") == "traffic_sign" and det.get("label") in ["traffic_sign", "sign"]:
                fine_label = await detector.classify_sign_rag(image_path, det["bbox"])
                det["label"] = fine_label
    
    # æ›´æ–°ç»Ÿè®¡
    for det in detections:
        cat = det.get("category", "unknown")
        if cat in stats:
            stats[cat] += 1
    
    # ä¿å­˜
    annotation = to_xanylabeling_format(detections, image_path)
    out_path = output_dir / f"{Path(image_path).stem}.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(annotation, f, ensure_ascii=False, indent=2)
    
    return (len(detections), None)


# ============================================================================
# Step 3: å¯è§†åŒ–
# ============================================================================

def generate_visualizations(frames_dir: Path, annotations_dir: Path, dataset_dir: Path) -> Path:
    """ç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡ï¼Œç›´æ¥è¾“å‡ºåˆ° dataset ç›®å½•"""
    from PIL import ImageDraw
    
    print(f"\nğŸ¨ Step 3: ç”Ÿæˆå¯è§†åŒ–")
    
    vis_dir = dataset_dir / "visualized"
    vis_dir.mkdir(parents=True, exist_ok=True)
    
    count = 0
    for json_path in sorted(annotations_dir.glob("*.json")):
        # æ”¯æŒ jpg å’Œ png ä¸¤ç§æ ¼å¼
        frame_path = None
        for ext in [".jpg", ".png"]:
            candidate = frames_dir / (json_path.stem + ext)
            if candidate.exists():
                frame_path = candidate
                break

        if not frame_path:
            continue
        
        img = Image.open(frame_path)
        draw = ImageDraw.Draw(img)
        
        with open(json_path) as f:
            data = json.load(f)
        
        for shape in data.get("shapes", []):
            pts = shape["points"]
            cat = shape.get("flags", {}).get("category", "unknown")
            label = shape["label"]
            
            color = COLORS.get(cat, (128, 128, 128))
            draw.rectangle([pts[0][0], pts[0][1], pts[1][0], pts[1][1]], 
                          outline=color, width=3)
            
            short_label = label[:20] + "..." if len(label) > 20 else label
            draw.text((pts[0][0], pts[0][1] - 15), short_label, fill=color)
        
        out_path = vis_dir / f"{json_path.stem}_vis.jpg"
        img.save(out_path)
        count += 1
        
        if count % 50 == 0:
            print(f"   å·²å¤„ç† {count} å¼ ...")
    
    print(f"   âœ… ç”Ÿæˆ {count} å¼ å¯è§†åŒ–å›¾ç‰‡")
    return vis_dir


# ============================================================================
# Step 4: æ‰“åŒ… Dataset
# ============================================================================

def generate_summary(annotations_dir: Path, video_name: str, frame_count: int) -> dict:
    """åˆ†ææ ‡æ³¨æ•°æ®å¹¶ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
    from collections import defaultdict
    
    stats = {
        "total_frames": frame_count,
        "annotated_frames": 0,
        "total_objects": 0,
        "categories": defaultdict(int),
        "subcategories": defaultdict(int),
    }
    
    for json_path in sorted(annotations_dir.glob("*.json")):
        with open(json_path, encoding="utf-8") as f:
            data = json.load(f)
        
        shapes = data.get("shapes", [])
        if shapes:
            stats["annotated_frames"] += 1
        
        for shape in shapes:
            stats["total_objects"] += 1
            category = shape.get("flags", {}).get("category", "unknown")
            stats["categories"][category] += 1
            label = shape.get("label", "")
            if label:
                stats["subcategories"][label] += 1
    
    return stats


def create_summary_markdown(stats: dict, video_name: str, fps: int, elapsed_time: float = None) -> str:
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æ€»ç»“æ–‡æ¡£"""
    from datetime import datetime
    
    lines = []
    lines.append(f"# ğŸ“Š æ•°æ®æ ‡æ³¨æ€»ç»“ - {video_name}")
    lines.append("")
    lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"**æŠ½å¸§ç‡**: {fps} FPS")
    lines.append(f"**æ ‡æ³¨æ–¹å¼**: å¼‚æ­¥å¹¶è¡Œ (asyncio + httpx)")
    if elapsed_time:
        minutes = int(elapsed_time // 60)
        seconds = elapsed_time % 60
        lines.append(f"**å¤„ç†è€—æ—¶**: {minutes}åˆ†{seconds:.1f}ç§’")
    lines.append("")
    
    lines.append("## ğŸ“ˆ æ¦‚è§ˆç»Ÿè®¡")
    lines.append("")
    lines.append(f"| æŒ‡æ ‡ | æ•°å€¼ |")
    lines.append(f"|------|------|")
    lines.append(f"| æ€»å¸§æ•° | {stats['total_frames']} |")
    lines.append(f"| æœ‰æ ‡æ³¨çš„å¸§ | {stats['annotated_frames']} |")
    lines.append(f"| ç©ºå¸§ï¼ˆæ— æ£€æµ‹ï¼‰ | {stats['total_frames'] - stats['annotated_frames']} |")
    lines.append(f"| æ€»æ£€æµ‹å¯¹è±¡ | {stats['total_objects']} |")
    if stats['annotated_frames'] > 0:
        lines.append(f"| å¹³å‡æ¯å¸§å¯¹è±¡æ•° | {stats['total_objects'] / stats['annotated_frames']:.2f} |")
    lines.append("")
    
    lines.append("## ğŸ·ï¸ ä¸»ç±»åˆ«åˆ†å¸ƒ")
    lines.append("")
    lines.append(f"| ç±»åˆ« | æ•°é‡ | å æ¯” |")
    lines.append(f"|------|------|------|")
    total = stats['total_objects'] or 1
    for cat, count in sorted(stats['categories'].items(), key=lambda x: -x[1]):
        percentage = count / total * 100
        lines.append(f"| {cat} | {count} | {percentage:.1f}% |")
    lines.append("")
    
    if stats['subcategories']:
        lines.append("## ğŸ” ç»†åˆ†ç±»åˆ« Top 20")
        lines.append("")
        lines.append(f"| æ ‡ç­¾ | æ•°é‡ |")
        lines.append(f"|------|------|")
        for label, count in sorted(stats['subcategories'].items(), key=lambda x: -x[1])[:20]:
            display_label = label[:50] + "..." if len(label) > 50 else label
            lines.append(f"| {display_label} | {count} |")
        lines.append("")
    
    lines.append("---")
    lines.append(f"*æ­¤æŠ¥å‘Šç”± video_to_dataset_async.py è‡ªåŠ¨ç”Ÿæˆ*")
    
    return "\n".join(lines)


def finalize_dataset(video_name: str, video_path: str, dataset_dir: Path, fps: int = 3, elapsed_time: float = None) -> Path:
    """ç”ŸæˆæŠ¥å‘Šå¹¶å®Œæˆ Datasetï¼ˆæ–‡ä»¶å·²ç›´æ¥ç”Ÿæˆåˆ°ç›®å½•ï¼Œæ— éœ€å¤åˆ¶ï¼‰"""
    import shutil
    
    print(f"\nğŸ“¦ Step 4: å®Œæˆ Dataset")
    
    # å¤åˆ¶è§†é¢‘åˆ° video å­ç›®å½•
    video_subdir = dataset_dir / "video"
    video_subdir.mkdir(parents=True, exist_ok=True)
    video_src = Path(video_path)
    video_dest = video_subdir / video_src.name
    if video_src.exists() and not video_dest.exists():
        shutil.copy(video_src, video_dest)
        print(f"   âœ… å¤åˆ¶è§†é¢‘")
    
    # ç»Ÿè®¡å·²æœ‰æ–‡ä»¶ï¼ˆæ”¯æŒ jpg å’Œ pngï¼‰
    frames_dir = dataset_dir / "frames"
    annotations_dir = dataset_dir / "annotations"
    vis_dir = dataset_dir / "visualized"

    frame_count = len(list(frames_dir.glob("*.jpg")) + list(frames_dir.glob("*.png"))) if frames_dir.exists() else 0
    ann_count = len(list(annotations_dir.glob("*.json"))) if annotations_dir.exists() else 0
    vis_count = len(list(vis_dir.glob("*.jpg"))) if vis_dir.exists() else 0
    
    print(f"   ğŸ“Š å¸§: {frame_count} | æ ‡æ³¨: {ann_count} | å¯è§†åŒ–: {vis_count}")
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print(f"   ğŸ“ ç”Ÿæˆæ ‡æ³¨æ€»ç»“æ–‡æ¡£...")
    stats = generate_summary(annotations_dir, video_name, frame_count)
    stats["processing_time"] = elapsed_time  # è®°å½•å¤„ç†æ—¶é—´
    summary_md = create_summary_markdown(stats, video_name, fps, elapsed_time)
    
    summary_path = dataset_dir / f"{video_name}_dataset_info.txt"
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write(summary_md)
    print(f"   âœ… ç”Ÿæˆ {video_name}_dataset_info.txt")
    
    # ä¿å­˜ JSON æ ¼å¼çš„ç»Ÿè®¡æ•°æ®
    stats_json = {
        "video_name": video_name,
        "total_frames": stats["total_frames"],
        "annotated_frames": stats["annotated_frames"],
        "total_objects": stats["total_objects"],
        "categories": dict(stats["categories"]),
        "subcategories": dict(stats["subcategories"]),
        "fps": fps,
        "processing_time": elapsed_time
    }
    stats_path = dataset_dir / "stats.json"
    with open(stats_path, "w", encoding="utf-8") as f:
        json.dump(stats_json, f, ensure_ascii=False, indent=2)
    print(f"   âœ… ç”Ÿæˆ stats.json")
    
    return dataset_dir


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

async def main_async():
    parser = argparse.ArgumentParser(description="å¼‚æ­¥è§†é¢‘åˆ°æ•°æ®é›†æµæ°´çº¿ï¼ˆæ”¯æŒå¤šæ¨¡å‹ï¼šGLM / Geminiï¼‰")
    parser.add_argument("--video", type=str, required=True, help="è§†é¢‘æ–‡ä»¶è·¯å¾„ (å¦‚ raw_data/videos/clips/D1/D1_000.mp4)")
    parser.add_argument("--name", type=str, default=None, help="è¾“å‡ºåç§° (é»˜è®¤ä½¿ç”¨è§†é¢‘æ–‡ä»¶å)")
    parser.add_argument("--output", type=str, default=None, help="è¾“å‡ºç›®å½• (é»˜è®¤ dataset_output)")
    parser.add_argument("--fps", type=int, default=1, help="æŠ½å¸§ç‡ (é»˜è®¤ 1)")
    parser.add_argument("--jpeg-frames", action="store_true", help="ä½¿ç”¨ JPEG q=2 æŠ½å¸§ï¼ˆçœç©ºé—´ï¼‰ï¼Œé»˜è®¤ä¸º PNG æ— æŸ")
    parser.add_argument("--all-frames", action="store_true", help="æå–æ‰€æœ‰å¸§ï¼ˆé»˜è®¤åªæå– I-frame å…³é”®å¸§ï¼Œç”»è´¨æœ€é«˜ï¼‰")
    parser.add_argument("--workers", type=int, default=15, help="å¹¶å‘æ•° (é»˜è®¤ 15)")
    parser.add_argument("--skip-visualize", action="store_true", help="è·³è¿‡å¯è§†åŒ–")
    parser.add_argument("--rag", action="store_true", default=True, help="å¯ç”¨ RAG äº¤é€šæ ‡å¿—ç»†ç²’åº¦åˆ†ç±» (é»˜è®¤å¯ç”¨)")
    parser.add_argument("--no-rag", dest="rag", action="store_false", help="ç¦ç”¨ RAG äº¤é€šæ ‡å¿—ç»†ç²’åº¦åˆ†ç±»")
    parser.add_argument("--clip-rag", action="store_true", help="(æ—§å‚æ•°) ä½¿ç”¨ CLIP å‘é‡æ£€ç´¢")
    parser.add_argument("--sign-classifier", type=str, default=None,
                        choices=["dinov2", "clip", "vlm"],
                        help="äº¤é€šæ ‡å¿—åˆ†ç±»å™¨: dinov2(æ¨è,æœ€å¼º), clip, vlm(é»˜è®¤)")
    parser.add_argument("--model", type=str, default="glm", 
                        choices=["glm", "gemini", "gemini-2.5-flash", "gemini-2.0-flash", "gemini-2.5-pro", "gemini-3-pro"], 
                        help="é€‰æ‹©æ¨¡å‹: glm, gemini-2.5-flash(æ¨è), gemini-2.0-flash, gemini-2.5-pro, gemini-3-pro (é»˜è®¤ glm)")
    args = parser.parse_args()
    
    video_path = Path(args.video)
    model_type = args.model
    model_config = MODEL_CONFIGS.get(model_type, MODEL_CONFIGS["glm"])
    
    # è‡ªåŠ¨ç¡®å®šè¾“å‡ºåç§°
    if args.name:
        output_name = args.name
    else:
        output_name = video_path.stem  # å¦‚ D1_000
    
    # æ ¹æ®æ¨¡å‹è·å–å¯¹åº”çš„ API Key
    api_key_env = model_config["api_key_env"]
    api_key = os.getenv(api_key_env)
    
    if not api_key:
        print(f"âŒ è¯·è®¾ç½® {api_key_env} ç¯å¢ƒå˜é‡")
        print(f"   export {api_key_env}='your_api_key'")
        return
    
    if not video_path.exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return
    
    # åˆ›å»º dataset ç›®å½•ï¼ˆæ‰€æœ‰æ–‡ä»¶ç›´æ¥è¾“å‡ºåˆ°è¿™é‡Œï¼‰
    output_base = Path(args.output) if args.output else DATASET_OUTPUT
    dataset_dir = output_base / f"{output_name}_dataset"
    dataset_dir.mkdir(parents=True, exist_ok=True)
    
    model_display = f"ğŸ”· {model_config['name']}" if model_type == "glm" else f"ğŸ”¶ {model_config['name']}"
    rag_display = "ğŸ¯ CLIP" if args.clip_rag else ("âœ… VLM" if args.rag else "âŒ ç¦ç”¨")
    frame_format = "JPEG q=2" if args.jpeg_frames else "PNG æ— æŸ"

    print("=" * 70)
    print(f"ğŸš€ å¼‚æ­¥è§†é¢‘æ ‡æ³¨æµæ°´çº¿ - {output_name}")
    print(f"   è§†é¢‘: {video_path}")
    print(f"   è¾“å‡º: {dataset_dir}")
    print(f"   æ¨¡å‹: {model_display} | æ ‡å¿—åˆ†ç±»: {rag_display}")
    print(f"   FPS: {args.fps} | å¸§æ ¼å¼: {frame_format} | å¹¶å‘: {args.workers}")
    print("=" * 70)
    
    start_time = time.time()
    
    # Step 1: æŠ½å¸§ï¼ˆç›´æ¥åˆ° dataset/framesï¼‰
    # é»˜è®¤ PNG æ— æŸï¼Œ--jpeg-frames æ—¶ä½¿ç”¨ JPEG
    # é»˜è®¤åªæå– I-frame å…³é”®å¸§ï¼Œ--all-frames æ—¶æå–æ‰€æœ‰å¸§
    lossless = not args.jpeg_frames
    keyframes_only = not args.all_frames
    frames_dir, _ = extract_frames(str(video_path), output_name, dataset_dir, args.fps, lossless=lossless, keyframes_only=keyframes_only)
    if not frames_dir:
        return
    
    # Step 2: æ ‡æ³¨ï¼ˆç›´æ¥åˆ° dataset/annotationsï¼‰
    # ç¡®å®šåˆ†ç±»å™¨ç±»å‹
    sign_classifier_type = args.sign_classifier
    if sign_classifier_type is None and args.clip_rag:
        sign_classifier_type = "clip"  # å…¼å®¹æ—§å‚æ•°
    
    annotations_dir = await run_labeling_async(
        frames_dir, dataset_dir, output_name, args.workers, api_key, 
        use_rag=args.rag, model_type=model_type, use_clip_rag=args.clip_rag,
        sign_classifier_type=sign_classifier_type
    )
    if not annotations_dir:
        return
    
    # Step 3: å¯è§†åŒ–ï¼ˆç›´æ¥åˆ° dataset/visualizedï¼‰
    if args.skip_visualize:
        print(f"\nâ­ï¸ è·³è¿‡å¯è§†åŒ–")
    else:
        generate_visualizations(frames_dir, annotations_dir, dataset_dir)
    
    # Step 4: ç”ŸæˆæŠ¥å‘Š
    total_time = time.time() - start_time
    finalize_dataset(output_name, str(video_path), dataset_dir, fps=args.fps, elapsed_time=total_time)
    
    print("\n" + "=" * 70)
    print(f"ğŸ‰ å®Œæˆï¼æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ ({total_time:.1f}ç§’)")
    print(f"ğŸ“ Dataset: {dataset_dir}/")
    print("=" * 70)


def main():
    asyncio.run(main_async())


if __name__ == "__main__":
    main()

