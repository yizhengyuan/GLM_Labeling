#!/usr/bin/env python3
"""
ğŸ¯ CLIP + Chroma äº¤é€šæ ‡å¿—åˆ†ç±»å™¨ï¼ˆçœŸæ­£çš„ RAGï¼‰

ä½¿ç”¨ CLIP æ¨¡å‹å°†æ ‡å¿—å›¾ç‰‡ç¼–ç ä¸ºå‘é‡ï¼Œå­˜å…¥ Chroma å‘é‡æ•°æ®åº“ï¼Œ
é€šè¿‡å›¾åƒç›¸ä¼¼åº¦æ£€ç´¢å®ç°ç²¾ç¡®çš„äº¤é€šæ ‡å¿—åˆ†ç±»ã€‚

ç”¨æ³•:
    # 1. é¦–æ¬¡è¿è¡Œï¼šå»ºç«‹å‘é‡åº“
    python scripts/clip_rag_classifier.py --build
    
    # 2. æµ‹è¯•åˆ†ç±»
    python scripts/clip_rag_classifier.py --test path/to/sign.jpg
    
    # 3. ä½œä¸ºæ¨¡å—å¯¼å…¥
    from scripts.clip_rag_classifier import CLIPSignClassifier
    classifier = CLIPSignClassifier()
    label, score = classifier.classify(image_path, bbox)
"""

import os
import sys
from pathlib import Path
from typing import List, Tuple, Optional
import uuid

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import numpy as np
from PIL import Image

# ============================================================================
# é…ç½®
# ============================================================================

# æ ‡å¿—å›¾ç‰‡ç›®å½•
SIGNS_DIR = Path("raw_data/signs")

# Chroma æ•°æ®åº“ç›®å½•
CHROMA_DB_DIR = Path("raw_data/chroma_signs_db")

# CLIP æ¨¡å‹é…ç½®ï¼ˆä½¿ç”¨æœ€å…ˆè¿›çš„ ViT-L/14ï¼‰
CLIP_MODEL_NAME = "ViT-L-14"
CLIP_PRETRAINED = "openai"  # æˆ– "laion2b_s32b_b82k" æ›´å¤§è§„æ¨¡é¢„è®­ç»ƒ

# 69 ä¸ªæ‘©æ‰˜è½¦å®‰å…¨ç›¸å…³æ ‡å¿—ï¼ˆä¸ sign_classifier_v2 ä¿æŒä¸€è‡´ï¼‰
MOTORCYCLE_SAFETY_SIGNS = [
    "No_motor_cycles_or_motor_tricycles",
    "No_motor_vehicles_except_motor_cyclists_and_motor_tricycles",
    "Parking_place_for_motor_cycles_only",
    "Speed_limit_(in_km_h)",
    "Variable_speed_limit_(in_km_h)",
    "Reduce_speed_now",
    "Keep_in_low_gear",
    "Use_low_gear",
    "Use_low_gear_for_distance_shown",
    "Slippery_road_ahead",
    "Loose_chippings_ahead",
    "Uneven_road_surface_ahead",
    "Road_hump_ahead",
    "Ramp_or_sudden_change_of_road_level_ahead",
    "Ramp_or_sudden_change_of_road_level",
    "Risk_of_falling_or_fallen_rocks_ahead",
    "Road_works_ahead",
    "Bend_to_left_ahead",
    "Double_bend_ahead_first_to_right",
    "Sharp_deviation_of_route_to_left",
    "Steep_hill_downwards_ahead",
    "Steep_hill_upwards_ahead",
    "Road_narrows_on_both_sides_ahead",
    "No_overtaking",
    "No_entry_for_all_vehicles",
    "No_entry_for_vehicles",
    "No_motor_vehicles",
    "No_stopping_at_any_time",
    "No_stopping",
    "One_way_traffic",
    "One_way_road_ahead",
    "Ahead_only",
    "Keep_right_(keep_left_if_symbol_reversed)",
    "Stop_and_give_way",
    "Give_way_to_traffic_on_major_road",
    "Distance_to__Stop__line",
    "Distance_to__Give_way__line",
    "Stop_or_give_way_ahead_(with_distance_to_line_ahead_given_below)",
    "Cross_roads_ahead",
    "T-junction_ahead",
    "Side_road_to_left_ahead",
    "Staggered_junction_ahead",
    "Traffic_merging_from_left",
    "Merging_into_main_traffic_on_left",
    "Two-way_traffic_ahead",
    "Two-way_traffic_across_a_one-way_road_ahead",
    "Traffic_lights_ahead",
    "Traffic_signals_ahead",
    "Red_light_camera_control_zone",
    "Red_light_speed_camera_ahead",
    "Prepare_to_stop_if_signalled_to_do_so",
    "Vehicles_must_stop_at_the_sign_(sign_used_by_police)",
    "Pedestrian_crossing_ahead",
    "Pedestrians_Ahead",
    "Pedestrian_on_or_crossing_road_ahead",
    "Children_ahead",
    "School_ahead",
    "Playground_ahead",
    "Cyclists_ahead",
    "Disabled_persons_ahead",
    "Visually_impaired_persons_ahead",
    "Traffic_Accident_blackspot_ahead",
    "Pedestrian_Accident_blackspot_ahead",
    "Fog_or_mist_ahead",
    "Restricted_headroom_ahead",
    "No_vehicles_over_height_shown_(including_load)",
    "No_vehicles_over_width_shown_(including_load)",
    "No_vehicles_over_gross_vehicle_weight_shown_(including_load)",
    "No_vehicles_over_axle_weight_shown_(including_load)",
]


# ============================================================================
# CLIP æ¨¡å‹å°è£…
# ============================================================================

class CLIPEncoder:
    """CLIP å›¾åƒç¼–ç å™¨"""
    
    def __init__(self, model_name: str = CLIP_MODEL_NAME, pretrained: str = CLIP_PRETRAINED):
        """
        åˆå§‹åŒ– CLIP æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œæ¨è ViT-L-14ï¼ˆæ›´å‡†ç¡®ï¼‰æˆ– ViT-B-32ï¼ˆæ›´å¿«ï¼‰
            pretrained: é¢„è®­ç»ƒæƒé‡æ¥æº
        """
        try:
            import open_clip
        except ImportError:
            raise ImportError("è¯·å®‰è£… open_clip: pip install open_clip_torch")
        
        print(f"ğŸ”§ åŠ è½½ CLIP æ¨¡å‹: {model_name} ({pretrained})")
        
        self.device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"   è®¾å¤‡: {self.device}")
        
        self.model, _, self.preprocess = open_clip.create_model_and_transforms(
            model_name, 
            pretrained=pretrained
        )
        self.model = self.model.to(self.device)
        self.model.eval()
        
        print(f"   âœ… CLIP æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def encode_image(self, image: Image.Image) -> np.ndarray:
        """
        ç¼–ç å•å¼ å›¾ç‰‡
        
        Args:
            image: PIL Image å¯¹è±¡
        
        Returns:
            å½’ä¸€åŒ–åçš„ç‰¹å¾å‘é‡ (numpy array)
        """
        with torch.no_grad():
            image_input = self.preprocess(image).unsqueeze(0).to(self.device)
            features = self.model.encode_image(image_input)
            features = features / features.norm(dim=-1, keepdim=True)
            return features.cpu().numpy().flatten()
    
    def encode_image_path(self, image_path: str) -> np.ndarray:
        """ä»è·¯å¾„åŠ è½½å¹¶ç¼–ç å›¾ç‰‡"""
        image = Image.open(image_path).convert("RGB")
        return self.encode_image(image)


# ============================================================================
# Chroma å‘é‡æ•°æ®åº“å°è£…
# ============================================================================

class SignVectorDB:
    """äº¤é€šæ ‡å¿—å‘é‡æ•°æ®åº“"""
    
    def __init__(self, db_dir: Path = CHROMA_DB_DIR, collection_name: str = "traffic_signs"):
        """
        åˆå§‹åŒ– Chroma æ•°æ®åº“
        
        Args:
            db_dir: æ•°æ®åº“å­˜å‚¨ç›®å½•
            collection_name: é›†åˆåç§°
        """
        try:
            import chromadb
            from chromadb.config import Settings
        except ImportError:
            raise ImportError("è¯·å®‰è£… chromadb: pip install chromadb")
        
        self.db_dir = Path(db_dir)
        self.db_dir.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨æŒä¹…åŒ–å®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(
            path=str(self.db_dir),
            settings=Settings(anonymized_telemetry=False)
        )
        
        # è·å–æˆ–åˆ›å»ºé›†åˆï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
        
        print(f"ğŸ“¦ Chroma æ•°æ®åº“: {self.db_dir}")
        print(f"   é›†åˆ: {collection_name} ({self.collection.count()} æ¡è®°å½•)")
    
    def add_signs(self, embeddings: List[np.ndarray], labels: List[str], image_paths: List[str]):
        """
        æ‰¹é‡æ·»åŠ æ ‡å¿—å‘é‡
        
        Args:
            embeddings: ç‰¹å¾å‘é‡åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        """
        ids = [f"sign_{i}_{label[:50]}" for i, label in enumerate(labels)]
        
        self.collection.add(
            ids=ids,
            embeddings=[e.tolist() for e in embeddings],
            metadatas=[{"label": label, "image_path": path} for label, path in zip(labels, image_paths)],
            documents=labels  # ç”¨äºæ–‡æœ¬æœç´¢å¤‡ç”¨
        )
        
        print(f"   âœ… æ·»åŠ  {len(labels)} ä¸ªæ ‡å¿—å‘é‡")
    
    def query(self, query_embedding: np.ndarray, top_k: int = 1) -> List[dict]:
        """
        æŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ ‡å¿—
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›å‰ k ä¸ªç»“æœ
        
        Returns:
            ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« label, score, image_path
        """
        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=top_k,
            include=["metadatas", "distances"]
        )
        
        output = []
        if results["metadatas"] and results["distances"]:
            for metadata, distance in zip(results["metadatas"][0], results["distances"][0]):
                # Chroma ä½¿ç”¨è·ç¦»ï¼Œä½™å¼¦ç›¸ä¼¼åº¦ = 1 - distance
                score = 1 - distance
                output.append({
                    "label": metadata["label"],
                    "score": score,
                    "image_path": metadata.get("image_path", "")
                })
        
        return output
    
    def clear(self):
        """æ¸…ç©ºé›†åˆ"""
        # åˆ é™¤å¹¶é‡å»ºé›†åˆ
        self.client.delete_collection("traffic_signs")
        self.collection = self.client.create_collection(
            name="traffic_signs",
            metadata={"hnsw:space": "cosine"}
        )
        print("   ğŸ—‘ï¸ å·²æ¸…ç©ºå‘é‡åº“")


# ============================================================================
# ä¸»åˆ†ç±»å™¨
# ============================================================================

class CLIPSignClassifier:
    """
    CLIP + Chroma äº¤é€šæ ‡å¿—åˆ†ç±»å™¨
    
    ç”¨æ³•:
        classifier = CLIPSignClassifier()
        
        # å»ºåº“ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
        classifier.build_database()
        
        # åˆ†ç±»
        label, score = classifier.classify("image.jpg", [100, 200, 150, 250])
    """
    
    def __init__(
        self, 
        signs_dir: Path = SIGNS_DIR,
        db_dir: Path = CHROMA_DB_DIR,
        use_69_signs: bool = True,
        similarity_threshold: float = 0.5
    ):
        """
        åˆå§‹åŒ–åˆ†ç±»å™¨
        
        Args:
            signs_dir: æ ‡å¿—å›¾ç‰‡ç›®å½•
            db_dir: å‘é‡æ•°æ®åº“ç›®å½•
            use_69_signs: æ˜¯å¦åªä½¿ç”¨ 69 ä¸ªæ‘©æ‰˜è½¦å®‰å…¨ç›¸å…³æ ‡å¿—
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è¿”å› "other"
        """
        self.signs_dir = Path(signs_dir)
        self.db_dir = Path(db_dir)
        self.use_69_signs = use_69_signs
        self.similarity_threshold = similarity_threshold
        
        # æ‡’åŠ è½½
        self._encoder = None
        self._db = None
    
    @property
    def encoder(self) -> CLIPEncoder:
        """æ‡’åŠ è½½ CLIP ç¼–ç å™¨"""
        if self._encoder is None:
            self._encoder = CLIPEncoder()
        return self._encoder
    
    @property
    def db(self) -> SignVectorDB:
        """æ‡’åŠ è½½å‘é‡æ•°æ®åº“"""
        if self._db is None:
            self._db = SignVectorDB(self.db_dir)
        return self._db
    
    def build_database(self, rebuild: bool = False):
        """
        å»ºç«‹å‘é‡æ•°æ®åº“
        
        Args:
            rebuild: æ˜¯å¦é‡å»ºï¼ˆæ¸…ç©ºç°æœ‰æ•°æ®ï¼‰
        """
        print("=" * 60)
        print("ğŸ—ï¸ å»ºç«‹äº¤é€šæ ‡å¿—å‘é‡æ•°æ®åº“")
        print("=" * 60)
        
        if rebuild:
            self.db.clear()
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
        if self.db.collection.count() > 0:
            print(f"   âš ï¸ æ•°æ®åº“å·²æœ‰ {self.db.collection.count()} æ¡è®°å½•ï¼Œè·³è¿‡å»ºåº“")
            print(f"   è‹¥éœ€é‡å»ºï¼Œè¯·ä½¿ç”¨ --rebuild å‚æ•°")
            return
        
        # è·å–æ ‡å¿—å›¾ç‰‡
        if self.use_69_signs:
            # åªä½¿ç”¨ 69 ä¸ªæ ¸å¿ƒæ ‡å¿—
            sign_names = MOTORCYCLE_SAFETY_SIGNS
            print(f"   ğŸ“‹ ä½¿ç”¨ 69 ä¸ªæ‘©æ‰˜è½¦å®‰å…¨ç›¸å…³æ ‡å¿—")
        else:
            # ä½¿ç”¨æ‰€æœ‰æ ‡å¿—
            sign_names = [f.stem for f in sorted(self.signs_dir.glob("*.png"))]
            print(f"   ğŸ“‹ ä½¿ç”¨å…¨éƒ¨ {len(sign_names)} ä¸ªæ ‡å¿—")
        
        # ç¼–ç å¹¶æ·»åŠ 
        embeddings = []
        labels = []
        paths = []
        
        print(f"\nğŸ”„ ç¼–ç æ ‡å¿—å›¾ç‰‡...")
        for i, name in enumerate(sign_names):
            # æŸ¥æ‰¾å¯¹åº”çš„å›¾ç‰‡æ–‡ä»¶
            image_path = self.signs_dir / f"{name}.png"
            if not image_path.exists():
                # å°è¯•æ¨¡ç³ŠåŒ¹é…
                matches = list(self.signs_dir.glob(f"{name}*.png"))
                if matches:
                    image_path = matches[0]
                else:
                    print(f"   âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡: {name}")
                    continue
            
            try:
                embedding = self.encoder.encode_image_path(str(image_path))
                embeddings.append(embedding)
                labels.append(name)
                paths.append(str(image_path))
                
                if (i + 1) % 10 == 0:
                    print(f"   å·²å¤„ç† {i + 1}/{len(sign_names)}")
                    
            except Exception as e:
                print(f"   âŒ ç¼–ç å¤±è´¥ {name}: {e}")
        
        # æ·»åŠ åˆ°æ•°æ®åº“
        print(f"\nğŸ’¾ æ·»åŠ åˆ°å‘é‡æ•°æ®åº“...")
        self.db.add_signs(embeddings, labels, paths)
        
        print(f"\nâœ… å»ºåº“å®Œæˆï¼å…± {len(labels)} ä¸ªæ ‡å¿—")
        print("=" * 60)
    
    def classify(
        self, 
        image_path: str, 
        bbox: List[int] = None,
        top_k: int = 1
    ) -> Tuple[str, float]:
        """
        åˆ†ç±»äº¤é€šæ ‡å¿—
        
        Args:
            image_path: åŸå›¾è·¯å¾„
            bbox: è¾¹ç•Œæ¡† [x1, y1, x2, y2]ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨æ•´å¼ å›¾ç‰‡
            top_k: è¿”å›å‰ k ä¸ªç»“æœ
        
        Returns:
            (label, score) - æ ‡ç­¾å’Œç›¸ä¼¼åº¦åˆ†æ•°
        """
        # åŠ è½½å¹¶è£å‰ªå›¾ç‰‡
        image = Image.open(image_path).convert("RGB")
        
        if bbox:
            padding = 5
            x1 = max(0, bbox[0] - padding)
            y1 = max(0, bbox[1] - padding)
            x2 = min(image.width, bbox[2] + padding)
            y2 = min(image.height, bbox[3] + padding)
            image = image.crop((x1, y1, x2, y2))
        
        # ç¼–ç 
        query_embedding = self.encoder.encode_image(image)
        
        # æŸ¥è¯¢
        results = self.db.query(query_embedding, top_k=top_k)
        
        if not results:
            return "other", 0.0
        
        top_result = results[0]
        
        # æ£€æŸ¥é˜ˆå€¼
        if top_result["score"] < self.similarity_threshold:
            return "other", top_result["score"]
        
        return top_result["label"], top_result["score"]
    
    def classify_with_details(
        self, 
        image_path: str, 
        bbox: List[int] = None,
        top_k: int = 3
    ) -> List[dict]:
        """
        åˆ†ç±»å¹¶è¿”å›è¯¦ç»†ç»“æœ
        
        Returns:
            Top-K ç»“æœåˆ—è¡¨
        """
        image = Image.open(image_path).convert("RGB")
        
        if bbox:
            padding = 5
            x1 = max(0, bbox[0] - padding)
            y1 = max(0, bbox[1] - padding)
            x2 = min(image.width, bbox[2] + padding)
            y2 = min(image.height, bbox[3] + padding)
            image = image.crop((x1, y1, x2, y2))
        
        query_embedding = self.encoder.encode_image(image)
        return self.db.query(query_embedding, top_k=top_k)


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="CLIP + Chroma äº¤é€šæ ‡å¿—åˆ†ç±»å™¨")
    parser.add_argument("--build", action="store_true", help="å»ºç«‹å‘é‡æ•°æ®åº“")
    parser.add_argument("--rebuild", action="store_true", help="é‡å»ºå‘é‡æ•°æ®åº“ï¼ˆæ¸…ç©ºç°æœ‰æ•°æ®ï¼‰")
    parser.add_argument("--test", type=str, help="æµ‹è¯•åˆ†ç±»ï¼ŒæŒ‡å®šå›¾ç‰‡è·¯å¾„")
    parser.add_argument("--bbox", type=str, help="è¾¹ç•Œæ¡†ï¼Œæ ¼å¼: x1,y1,x2,y2")
    parser.add_argument("--all-signs", action="store_true", help="ä½¿ç”¨å…¨éƒ¨ 188 ä¸ªæ ‡å¿—ï¼ˆé»˜è®¤åªç”¨ 69 ä¸ªï¼‰")
    parser.add_argument("--top-k", type=int, default=3, help="è¿”å›å‰ K ä¸ªç»“æœï¼ˆé»˜è®¤ 3ï¼‰")
    args = parser.parse_args()
    
    classifier = CLIPSignClassifier(use_69_signs=not args.all_signs)
    
    if args.build or args.rebuild:
        classifier.build_database(rebuild=args.rebuild)
    
    if args.test:
        print("\n" + "=" * 60)
        print(f"ğŸ” æµ‹è¯•åˆ†ç±»: {args.test}")
        print("=" * 60)
        
        bbox = None
        if args.bbox:
            bbox = [int(x) for x in args.bbox.split(",")]
            print(f"   è¾¹ç•Œæ¡†: {bbox}")
        
        results = classifier.classify_with_details(args.test, bbox, top_k=args.top_k)
        
        print(f"\nğŸ“Š Top-{args.top_k} ç»“æœ:")
        for i, r in enumerate(results):
            emoji = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰"
            print(f"   {emoji} {r['label']}")
            print(f"      ç›¸ä¼¼åº¦: {r['score']:.4f}")
        
        # æœ€ç»ˆæ ‡ç­¾
        label, score = classifier.classify(args.test, bbox)
        print(f"\nğŸ·ï¸ æœ€ç»ˆæ ‡ç­¾: {label} (score: {score:.4f})")
    
    if not args.build and not args.rebuild and not args.test:
        parser.print_help()


if __name__ == "__main__":
    main()

